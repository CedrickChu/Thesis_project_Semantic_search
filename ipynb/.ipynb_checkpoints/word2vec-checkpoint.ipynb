{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Word2Vec\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m word_tokenize, sent_tokenize\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import nltk\n",
    "import pymongo\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from typing import List\n",
    "import numpy as np\n",
    "import gensim.models as gm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to MongoDB\n",
    "client = pymongo.MongoClient(\"mongodb+srv://cedrickchu123:lzuaguRde81CZVuD@cluster0.75dzsfe.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0\")\n",
    "db = client.ThesisProject\n",
    "collection = db.Thesis_Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load stopwords\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text: str) -> List[str]:\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['artificial',\n",
       " 'intelligence',\n",
       " 'ai',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'ml',\n",
       " 'becoming',\n",
       " 'increasingly',\n",
       " 'ubiquitous',\n",
       " 'everyday',\n",
       " 'lives',\n",
       " 'despite',\n",
       " 'rapid',\n",
       " 'pace',\n",
       " 'developments',\n",
       " 'debates',\n",
       " 'around',\n",
       " 'responsible',\n",
       " 'ai',\n",
       " 'ai',\n",
       " 'go']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_text(collection.find_one()['abstract'][:200])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# # def sentence_tokenization(text: str) -> List[List[str]]:\n",
    "# #     tokenized_sentences = [word_tokenize(sentence) for sentence in sent_tokenize(text)]\n",
    "# #     return tokenized_sentences\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence_tokenization(collection.find_one()['abstract'][:200])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_corpus(text: str) -> List[str]:\n",
    "#     word_tokens = preprocess_text(text)\n",
    "#     sentence_tokens = sentence_tokenization(text)\n",
    "#     corpus = word_tokens + [word for sentence in sentence_tokens for word in sentence]\n",
    "#     return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_word2vec_model(corpus, num_epochs=10):\n",
    "    VECTOR_SIZE = 400\n",
    "    WINDOW = 40\n",
    "    MIN_COUNT = 2\n",
    "    SG = 4\n",
    "    model = Word2Vec(\n",
    "        sentences=[corpus], \n",
    "        vector_size=VECTOR_SIZE,\n",
    "        window=WINDOW,\n",
    "        min_count=MIN_COUNT,\n",
    "        sg=SG,\n",
    "        epochs=num_epochs\n",
    "    )\n",
    "    model.save('../models/word2vec_model.gensim')\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_abstracts = [doc['abstract'] for doc in collection.find()]\n",
    "all_titles = [doc['title'] for doc in collection.find()]\n",
    "all_keywords = [doc['keywords'] for doc in collection.find()]\n",
    "\n",
    "abstract_corpus = preprocess_text(\" \".join(all_abstracts))\n",
    "title_corpus = preprocess_text(\" \".join(all_titles))\n",
    "keywords_corpus = preprocess_text(\" \".join(all_keywords))\n",
    "\n",
    "all_tokens = abstract_corpus + title_corpus + keywords_corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc_vector(model, tokens):\n",
    "    vector = np.zeros(model.vector_size)\n",
    "    count = 0\n",
    "    for token in tokens:\n",
    "        if token in model.wv:\n",
    "            vector += model.wv[token]\n",
    "            count += 1\n",
    "    if count != 0:\n",
    "        vector /= count\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = train_word2vec_model(all_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model = gm.Word2Vec.load('../models/word2vec_model.gensim')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc_vector(model, tokens):\n",
    "    vector = np.zeros(model.vector_size)\n",
    "    count = 0\n",
    "    for token in tokens:\n",
    "        if token in model.wv:\n",
    "            vector += model.wv[token]\n",
    "            count += 1\n",
    "    if count != 0:\n",
    "        vector /= count\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.0253928  -0.03758292 -0.08837505 -0.04319915 -0.05274536  0.06954607\n",
      "  0.10446508 -0.02371051 -0.06254005  0.00300196  0.06490836  0.02708401\n",
      " -0.05347076 -0.01380235 -0.05826704  0.06091438 -0.06052105 -0.20271428\n",
      "  0.0216757   0.03228148  0.07644486  0.12259084  0.06480087  0.03281801\n",
      " -0.04893982  0.01191932 -0.10138899  0.02634321 -0.10317191  0.13886816\n",
      "  0.02964793  0.02891547  0.13914282 -0.03068939 -0.02708527  0.013894\n",
      "  0.15207835 -0.11466507  0.05410168 -0.22024774 -0.08206716  0.19339929\n",
      "  0.06229859 -0.0059548  -0.01163584 -0.08551034 -0.01490023  0.03058957\n",
      " -0.03374767  0.15836123  0.02103346 -0.06929488 -0.00518588 -0.00741289\n",
      "  0.08885724 -0.13330847  0.00534961 -0.07590371  0.06229144 -0.00068373\n",
      "  0.11581437  0.04407752  0.09269728  0.11787795 -0.04342002  0.01779208\n",
      "  0.0180223  -0.14254863  0.04707011  0.05801975  0.11560648  0.02900316\n",
      "  0.04640327 -0.00971275 -0.14465088 -0.14337143 -0.04686711  0.04969303\n",
      "  0.01803704  0.04299772  0.03007823 -0.05570075  0.10265339  0.02745421\n",
      "  0.02338333 -0.0348685  -0.01595957  0.07750264  0.1031831   0.0800305\n",
      " -0.13403083  0.04807055  0.04343654  0.04608946 -0.04271763 -0.11620689\n",
      " -0.10025411 -0.06724316  0.16793559 -0.12616136  0.02014848  0.00134093\n",
      " -0.14512023 -0.09317825 -0.00156421 -0.04200324  0.00632241 -0.00048253\n",
      " -0.00742142  0.03998399  0.10997986 -0.07262096  0.06594846 -0.00902131\n",
      "  0.0713478  -0.0829116  -0.03687027  0.0810902  -0.02338627  0.07061396\n",
      "  0.02571595 -0.01889653 -0.07160509 -0.07333007 -0.01111047  0.02657921\n",
      " -0.04730628 -0.0372882  -0.03641962 -0.0603486  -0.04249964 -0.05322916\n",
      "  0.11014459  0.10655471  0.05938836 -0.15679585  0.02518294 -0.0242644\n",
      " -0.11892212 -0.03323191 -0.0865935   0.00442101 -0.02276668  0.01970625\n",
      " -0.06836457 -0.0670236  -0.03194754  0.05497718 -0.01035902  0.06345457\n",
      " -0.11725007 -0.01550946  0.00315806  0.05074395 -0.13802934  0.01406987\n",
      "  0.04112733  0.04420908 -0.03749053  0.10994544  0.00408308  0.02221587\n",
      " -0.08498921  0.01045056  0.09008169 -0.05507906 -0.1328278   0.00031798\n",
      "  0.02362605 -0.00175672 -0.05416091  0.01426898  0.05880789 -0.00498618\n",
      " -0.06442384 -0.05176094  0.17974953 -0.03973388 -0.01664607  0.01111208\n",
      " -0.10441613 -0.01058902 -0.092124    0.03961511 -0.05365302  0.09742218\n",
      "  0.04586554  0.08637892  0.07620098 -0.05212706  0.05692646 -0.03099125\n",
      "  0.00813353  0.0155614   0.03232512 -0.05081764  0.00913479  0.06145462\n",
      " -0.13311716  0.07904154 -0.03137921  0.11898561  0.01781769 -0.12049232\n",
      " -0.09307511 -0.02189215 -0.05458182  0.01972745  0.1028446   0.04859079\n",
      "  0.02236461 -0.02112506  0.09243548  0.03435537  0.02047688  0.10595578\n",
      " -0.0050544  -0.08041766  0.11184438 -0.03000934  0.0654692  -0.02846037\n",
      " -0.03502562  0.06141269 -0.03494882 -0.02255957 -0.05909644  0.00232404\n",
      "  0.00799387 -0.07043888  0.12601264 -0.17300083  0.03547397 -0.07162577\n",
      "  0.06588167  0.04288194  0.014807    0.03772393 -0.09962171  0.03531149\n",
      "  0.13900446  0.04483124 -0.13008834  0.03871806 -0.0172804  -0.00261896\n",
      " -0.03898316 -0.0538473  -0.01383824 -0.00781824  0.12456993  0.05090528\n",
      " -0.01139361 -0.04971443 -0.13210626  0.03890867 -0.07991621 -0.02206011\n",
      "  0.04957152 -0.04950908 -0.00495527 -0.03673295 -0.13689314  0.0446776\n",
      " -0.05021111 -0.06994042  0.10738482 -0.03516019 -0.00973704  0.02120302\n",
      "  0.00627431 -0.03054177 -0.03673998 -0.09799945  0.07060284 -0.05542319\n",
      "  0.04808855 -0.12573464 -0.0128243  -0.10705108 -0.04551451 -0.06502563\n",
      " -0.06589727  0.0769379  -0.07387684  0.03337732 -0.02365062 -0.11940725\n",
      "  0.09103249  0.06165955  0.07177845 -0.14149613  0.09159898 -0.17701578\n",
      "  0.01171613  0.06754457  0.17045603 -0.09592179  0.06787463 -0.01518678\n",
      " -0.05529591 -0.04015518  0.03998658  0.07293877  0.10903295 -0.13626282\n",
      "  0.02142823 -0.11468628 -0.01007635  0.0872776   0.01770687 -0.08380989\n",
      "  0.08242068 -0.03381873  0.04817385 -0.02482026  0.01109919 -0.10119859\n",
      " -0.04938337  0.02627557  0.01035949 -0.0452169   0.06392581  0.01440159\n",
      "  0.01250659  0.22367597 -0.1452525  -0.03417621  0.0848343   0.06184122\n",
      " -0.06117465  0.06287918 -0.01979716 -0.08092774 -0.0332104   0.14947394\n",
      "  0.06236942 -0.0167706   0.00490233 -0.10503554  0.00177877 -0.10448286\n",
      " -0.05195196  0.04886175 -0.05998676  0.04122745 -0.01489571  0.08066146\n",
      " -0.03613039 -0.05280723  0.03385883 -0.00915704  0.02471968  0.11660881\n",
      "  0.04347988  0.09636971  0.08998442  0.11216579  0.09008806  0.03887896\n",
      "  0.02501278  0.10891705 -0.05861817  0.10530151  0.06851527 -0.01455386\n",
      " -0.04060639 -0.0382549  -0.04452338  0.13913608 -0.08922453  0.04115276\n",
      " -0.13805268  0.0132665  -0.02414734  0.00782599 -0.21877099  0.01762137\n",
      " -0.03329233  0.20725263  0.02476863  0.05299264  0.05873648  0.03996884\n",
      " -0.01037942 -0.03675479 -0.08313277 -0.11322851 -0.08347175 -0.01112457\n",
      " -0.04192394 -0.02155698 -0.10119967 -0.10035948 -0.01766572 -0.05762913\n",
      "  0.00627481 -0.03524473 -0.17121846 -0.1620261 ]\n"
     ]
    }
   ],
   "source": [
    "abstract_one = [doc['abstract'] for doc in collection.find()][0]\n",
    "example_abstract = preprocess_text(abstract_one)\n",
    "abstract_vector = get_doc_vector(load_model,example_abstract)\n",
    "\n",
    "print(abstract_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_query(text: str) -> List[str]:\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    query = [word for word in tokens if word not in stop_words]\n",
    "    return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query_vector(word2vec_model, query_tokens):\n",
    "    valid_tokens = [word for word in query_tokens if word in word2vec_model.wv]\n",
    "\n",
    "    if not valid_tokens:\n",
    "        return np.zeros(word2vec_model.vector_size)\n",
    "\n",
    "    query_vector = np.mean([word2vec_model.wv[word] for word in valid_tokens], axis=0)\n",
    "    return query_vector\n",
    "\n",
    "# def get_query_vector(model, tokens):\n",
    "#     vector = np.zeros(model.vector_size)\n",
    "#     count = 0\n",
    "#     for token in tokens:\n",
    "#         if token in model.wv:\n",
    "#             vector += model.wv[token]\n",
    "#             count += 1\n",
    "#     if count != 0:\n",
    "#         vector /= count\n",
    "#     return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def calculate_cosine_similarity(vec1, vec2):\n",
    "    return cosine_similarity([vec1], [vec2])[0][0]\n",
    "\n",
    "# def calculate_cosine_similarity(vec1, vec2):\n",
    "#     return np.dot(vec1, vec2) / (np.linalg.norm(query_vector) * np.linalg.norm(vec2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def find_most_relevant_docs(query_text, load_model):\n",
    "    query_tokens = preprocess_text(query_text)\n",
    "    query_vector = get_query_vector(load_model, query_tokens)\n",
    "    most_relevant_docs = []\n",
    "\n",
    "    for doc in collection.find():\n",
    "        doc_text = doc['abstract']\n",
    "        title = doc['title']\n",
    "        author = doc['author']\n",
    "        date_published = doc['date_published']\n",
    "        \n",
    "        title_tokens = preprocess_text(title)\n",
    "        title_vector = get_doc_vector(load_model, title_tokens)\n",
    "        \n",
    "        doc_tokens = preprocess_text(doc_text)\n",
    "        doc_vector = get_doc_vector(load_model, doc_tokens)\n",
    "        \n",
    "        combined_vector = np.concatenate((title_vector, doc_vector))\n",
    "\n",
    "\n",
    "        if np.all(combined_vector == 0) or np.all(query_vector == 0):\n",
    "            continue\n",
    "\n",
    "        similarity = cosine_similarity(query_vector.reshape(1, -1), doc_vector.reshape(1, -1))[0, 0]\n",
    "\n",
    "        if not np.isnan(similarity):\n",
    "            most_relevant_docs.append({'_id': doc['_id'], 'title': title, 'author': author,'date_published': date_published,'similarity': similarity, 'abstract': doc_text})\n",
    "\n",
    "    most_relevant_docs = sorted(most_relevant_docs, key=lambda x: x['similarity'], reverse=True)\n",
    "    return most_relevant_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document ID: 65e694c7afc3d58f1938551e\n",
      "Title: Machine Learning: A maturing field\n",
      "Similarity: 0.9421257840136774\n",
      "Abstract:\n",
      "With this volume I complete my four-year term as executive editor of Machine Learning,\n",
      "and Tom Dietterich, who has been co-executive editor with me recently, takes over the\n",
      "helm--or starts serving his sentence, depending upon one's point of view. Let me take this\n",
      "opportunity to make a few reflections about the state of the field; past, present and future,\n",
      "based on personal observations.\n",
      "A decade ago machine learning was regrouping from the rather uneventful 1970s. The\n",
      "first machine learning workshop was held in 1980 at Carnegie Mellon University with some\n",
      "two dozen participants and photocopied preprints. Shortly thereafter we started preparing\n",
      "the first machine learning book, and I was in charge of finding a publication venue. However\n",
      "the title \"Machine Learning\" raised skeptical eyebrows in publishers. By \"machine learning\" did we not really mean learning about machines rather than learning by machines?\n",
      "Couldn't we think of something more scientific-sounding to call the book? And anyway\n",
      "hadn't Minsky and Papert debunked this learning nonsense? Since it proved difficult to\n",
      "explain the difference between linear perceptrons and symbolic learning to those publishers\n",
      "(who shall go unnamed, to protect the guilty), we approached Nils Nilsson and his Tioga\n",
      "Press. Nils embraced the project with foresight and enthusiasm, and the rest, as they say\n",
      "in the tired cliche, is history.\n",
      "\n",
      "==================================================\n",
      "\n",
      "Document ID: 65e6914803a1e16834fb92d2\n",
      "Title: Communicating Ethics across the AI Ecosystem\n",
      "Similarity: 0.9393514107130049\n",
      "Abstract:\n",
      "Artificial Intelligence (AI) and machine learning (ML) are becoming increasingly ubiquitous in our everyday lives. Despite the rapid pace of these developments, debates around responsible AI and AI governance are still maturing. This paper outlines the growth of the ethical AI movement internationally and nationally to provide context for the CAIDG’s Ethics Hub Initiative. It touches on open questions in the field and discusses how these questions influenced the trajectory of the Hub’s research and methodologies.\n",
      "The paper reflects on the piloted empirical exercises and charts how research questions developed and changed over time. Along the way, we identified themes of power and resource differentials, as well as communication breakdowns that shaped the latter stages of our project.\n",
      "Given the nature of our pilot project, our findings are not indicative or generalizable of the entire field in Singapore. Nonetheless, they represent a meaningful snapshot of how debates around ethical AI and its operationalization are unfolding in Singapore and beyond. Ultimately, we encourage more focused research on the development and design of institutional incentive mechanisms to foster greater cohesion around responsible AI development and deployment norms.\n",
      "\n",
      "==================================================\n",
      "\n",
      "Document ID: 65e692c8afc3d58f1938551c\n",
      "Title: Automatic Facial Expression Analysis\n",
      "Similarity: 0.9306604064846398\n",
      "Abstract:\n",
      "Automatic Facial Expression Recognition systems have come a long way since the earliest approaches in the early 1970s. We are now at a point where the earliest systems are commercially applied, most notably the smile detectors in digital cameras. But although facial expression recognition is maturing as a research field, it is far from finished. New techniques continue to be developed on all aspects of the processing pipeline: from face detection, via feature extraction to machine learning. Nor is the field blind to the progress made in the social sciences with respect to emotion theory. Gone are the days that people only tried to detect six discrete expressions that were turned-on or off like the switching of lights. The theory of Social Signal Processing now complements classical emotion theory, and modern approaches dissect an expression into its temporal phases, analyse intensity, symmetry, micro-expressions and dynamic differences between morphologically similar expressions. Brave new worlds are opened up—Automatic Facial Expression Analysis is poised to revolutionalise medicine with the advent of behaviomedics, gaming with enriched player–non-player interactions, teleconference meetings with automatic trust and engagement analysis, and human–robot interaction with robots displaying actual empathy.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Document ID: 65e94690f9ec29cf9fcd390e\n",
      "Title: Cost-Effectiveness in Health and Medicine (2nd edn)\n",
      "Similarity: 0.8311927966610954\n",
      "Abstract:\n",
      "As healthcare costs rise in the United States, debate is ongoing over how to obtain better value for dollars spent. In this context, the use of cost-effectiveness analysis (CEA) is more compelling than ever. This book, written by the Second Panel on Cost-Effectiveness in Health and Medicine, reviews key concepts and analytic challenges in CEA. The authors endorse the original Panel’s concept of a reference case and support its recommendation that analysts take a broad societal perspective; in addition, they recommend a healthcare sector perspective for a second reference case, as well as an important new framework, the Impact Inventory, for detailing costs and effects. The revisions draw on advances in the field and include three new chapters that capture research on decision modeling, methods for evidence synthesis, and ethical considerations. The volume also includes two new worked examples (Appendix A and Appendix B) to illustrate ways to implement the authors’ recommendations.\n",
      "\n",
      "==================================================\n",
      "\n",
      "Document ID: 65fe5342fac1d4e4624fb65c\n",
      "Title: Result candidate total religious.\n",
      "Similarity: 0.8089901308490837\n",
      "Abstract:\n",
      "Hit ago mean tonight. Hot record trip technology. Analysis fish piece top the share.\n",
      "\n",
      "==================================================\n",
      "\n",
      "Document ID: 65fe5342fac1d4e4624fb666\n",
      "Title: Message build decade four night either begin.\n",
      "Similarity: 0.7717600122222044\n",
      "Abstract:\n",
      "Reality trade nice among. Fill nation wish second put specific top.\n",
      "\n",
      "==================================================\n",
      "\n",
      "Document ID: 65fe5342fac1d4e4624fb665\n",
      "Title: If game hot participant again break.\n",
      "Similarity: 0.7279944590193148\n",
      "Abstract:\n",
      "Area him address work student write. Two month value financial.\n",
      "\n",
      "==================================================\n",
      "\n",
      "Document ID: 65e946eff9ec29cf9fcd390f\n",
      "Title: Beyond the bedside: Clinicians as guardians of public health, medicine and science\n",
      "Similarity: 0.6974006053187092\n",
      "Abstract:\n",
      "This abstract delves into a conceptual framework that envisions clinicians not only as caregivers at the bedside but as pivotal guardians of public health, medicine, and science. The traditional role of clinicians has predominantly centered on individual patient care, yet this paradigm proposes an expansion of their responsibilities to encompass broader societal well-being. By exploring the intersections of clinical practice, public health initiatives, and scientific advancements, this paradigm shift aims to harness the expertise and influence of clinicians to address systemic health challenges.\n",
      "\n",
      "The proposed framework emphasizes the multifaceted roles clinicians can play in shaping public health policies, advancing medical research, and fostering a culture of evidence-based practice. Drawing on case studies and theoretical models, this abstract highlights the potential impact of empowering clinicians to contribute actively to public health discourse, policy formulation, and community education. It underscores the importance of integrating clinical knowledge with scientific inquiry, thereby fostering a more comprehensive approach to healthcare that transcends the confines of individual patient interactions.\n",
      "\n",
      "==================================================\n",
      "\n",
      "Document ID: 65fe5342fac1d4e4624fb65a\n",
      "Title: Next nation under it position kitchen relationship.\n",
      "Similarity: 0.6789801195358873\n",
      "Abstract:\n",
      "Book trade wait when. Room strategy what though water. Probably key force fire practice human while.\n",
      "\n",
      "==================================================\n",
      "\n",
      "Document ID: 65fe5342fac1d4e4624fb660\n",
      "Title: Produce notice long born.\n",
      "Similarity: 0.6636002657756954\n",
      "Abstract:\n",
      "Task once suggest treatment walk send. Manager me eat wind about determine sense official. Car he new attorney. Case remain edge method issue year.\n",
      "\n",
      "==================================================\n",
      "\n",
      "Document ID: 65fe5342fac1d4e4624fb661\n",
      "Title: Ahead oil civil a hour development.\n",
      "Similarity: 0.6611395395648054\n",
      "Abstract:\n",
      "Protect town executive visit person there. Bag own only pressure cover whatever sort.\n",
      "\n",
      "==================================================\n",
      "\n",
      "Document ID: 65e69688afc3d58f19385523\n",
      "Title: Towards Deep Learning Models Resistant to Adversarial Attacks\n",
      "Similarity: 0.6434146781347965\n",
      "Abstract:\n",
      "Recent work has demonstrated that deep neural networks are vulnerable to adversarial examples---inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. In fact, some of the latest findings suggest that the existence of adversarial attacks may be an inherent weakness of deep learning models. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much of the prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against any adversary. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest the notion of security against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models. Code and pre-trained models are available at this https URL and this https URL.\n",
      "\n",
      "==================================================\n",
      "\n",
      "Document ID: 65fe5342fac1d4e4624fb65d\n",
      "Title: Police third must themselves rate thousand.\n",
      "Similarity: 0.6394798125494178\n",
      "Abstract:\n",
      "Stage huge small different box some positive. Project sense away sort task. Whole social property camera sure. Cultural course number.\n",
      "\n",
      "==================================================\n",
      "\n",
      "Document ID: 65fe5342fac1d4e4624fb657\n",
      "Title: Quickly evening man admit form series performance.\n",
      "Similarity: 0.6381583201445542\n",
      "Abstract:\n",
      "Mention easy site among we recognize land medical. Modern president white soon. Much war stay sing similar above father fish.\n",
      "\n",
      "==================================================\n",
      "\n",
      "Document ID: 65fe5342fac1d4e4624fb662\n",
      "Title: Plan evidence include recently stay budget toward admit.\n",
      "Similarity: 0.6252351036747206\n",
      "Abstract:\n",
      "Teacher recent down true seem trial. Stock force because spend assume analysis. Above community action large value.\n",
      "\n",
      "==================================================\n",
      "\n",
      "Document ID: 65fe5342fac1d4e4624fb659\n",
      "Title: Local job five account.\n",
      "Similarity: 0.6214506297625928\n",
      "Abstract:\n",
      "Ten will read international health course study exactly. Contain prepare save partner central crime. Join could affect cut picture ever.\n",
      "\n",
      "==================================================\n",
      "\n",
      "Document ID: 65e69605afc3d58f19385521\n",
      "Title: Deep learning\n",
      "Similarity: 0.6087432811925931\n",
      "Abstract:\n",
      "Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Document ID: 65fe5342fac1d4e4624fb664\n",
      "Title: Quickly soon executive who economy somebody light.\n",
      "Similarity: 0.6041843576693093\n",
      "Abstract:\n",
      "Answer hospital better compare how than discover. Wife recent final probably hospital deep series. Respond example use work. Score expert turn generation forget box under.\n",
      "\n",
      "==================================================\n",
      "\n",
      "Document ID: 65fe5342fac1d4e4624fb654\n",
      "Title: Evidence serious tell painting why American.\n",
      "Similarity: 0.5986432215890161\n",
      "Abstract:\n",
      "Study sea suggest opportunity majority. Necessary occur they least election movement expect on. Top them trouble create happen any. Discover hair century see business ground lose.\n",
      "\n",
      "==================================================\n",
      "\n",
      "Document ID: 65fe5342fac1d4e4624fb653\n",
      "Title: Ever career buy or.\n",
      "Similarity: 0.585326818097269\n",
      "Abstract:\n",
      "Report hair dog yourself. Tend we drive music message art. Minute each word reach test lot. Of smile positive purpose.\n",
      "\n",
      "==================================================\n",
      "\n",
      "Document ID: 65fe5342fac1d4e4624fb65f\n",
      "Title: Only late nearly let least everyone.\n",
      "Similarity: 0.5834042915836305\n",
      "Abstract:\n",
      "Great federal step. Sense plant if during career affect why. Deal series home century political voice. Area everyone candidate crime another structure. Stock run respond natural provide.\n",
      "\n",
      "==================================================\n",
      "\n",
      "Document ID: 65fe5342fac1d4e4624fb663\n",
      "Title: Bring heavy herself model poor task.\n",
      "Similarity: 0.5833971727865721\n",
      "Abstract:\n",
      "Authority mission assume never large. Ok significant result firm. Happy something space itself police. Station game drug strong.\n",
      "\n",
      "==================================================\n",
      "\n",
      "Document ID: 65e69647afc3d58f19385522\n",
      "Title: A survey on Image Data Augmentation for Deep Learning\n",
      "Similarity: 0.5793488601884299\n",
      "Abstract:\n",
      "Deep convolutional neural networks have performed remarkably well on many Computer Vision tasks. However, these networks are heavily reliant on big data to avoid overfitting. Overfitting refers to the phenomenon when a network learns a function with very high variance such as to perfectly model the training data. Unfortunately, many application domains do not have access to big data, such as medical image analysis. This survey focuses on Data Augmentation, a data-space solution to the problem of limited data. Data Augmentation encompasses a suite of techniques that enhance the size and quality of training datasets such that better Deep Learning models can be built using them. The image augmentation algorithms discussed in this survey include geometric transformations, color space augmentations, kernel filters, mixing images, random erasing, feature space augmentation, adversarial training, generative adversarial networks, neural style transfer, and meta-learning. The application of augmentation methods based on GANs are heavily covered in this survey. In addition to augmentation techniques, this paper will briefly discuss other characteristics of Data Augmentation such as test-time augmentation, resolution impact, final dataset size, and curriculum learning. This survey will present existing methods for Data Augmentation, promising developments, and meta-level decisions for implementing Data Augmentation. Readers will understand how Data Augmentation can improve the performance of their models and expand limited datasets to take advantage of the capabilities of big data.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Document ID: 65e696c5afc3d58f19385524\n",
      "Title: An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\n",
      "Similarity: 0.5562722610317089\n",
      "Abstract:\n",
      "While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.\n",
      "\n",
      "==================================================\n",
      "\n",
      "Document ID: 65fe5342fac1d4e4624fb656\n",
      "Title: Imagine all usually strong.\n",
      "Similarity: 0.5485345110243052\n",
      "Abstract:\n",
      "Affect play third bill outside. Century game career participant customer eight check. Talk population old government current nice inside treatment.\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query_text = \"give me research about machine learning\"\n",
    "\n",
    "if query_text:\n",
    "    most_relevant_docs = find_most_relevant_docs(query_text, load_model)\n",
    "    \n",
    "    num_abstracts_to_print = min(25, len(most_relevant_docs))\n",
    "    for i in range(num_abstracts_to_print):\n",
    "        current_doc = most_relevant_docs[i]\n",
    "        print(f\"Document ID: {current_doc['_id']}\")\n",
    "        print(f\"Title: {current_doc['title']}\")\n",
    "        print(f\"Similarity: {current_doc['similarity']}\")\n",
    "        print(f\"Abstract:\\n{current_doc['abstract']}\")\n",
    "        print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for 'Object':\n",
      "[ 1.43212024e-02 -4.90436256e-02 -1.09686203e-01 -8.26093182e-02\n",
      " -1.08329698e-01  7.55029917e-02  2.91749656e-01  8.26767180e-03\n",
      " -9.34493691e-02 -1.15298545e-02  1.16078727e-01  5.89123741e-02\n",
      " -6.01173043e-02  8.29657838e-02 -1.29774317e-01  1.33065730e-01\n",
      " -5.37581891e-02 -2.82771260e-01  3.64694744e-02  7.56042674e-02\n",
      "  5.54740243e-02  1.95510432e-01  1.30149081e-01  8.31904858e-02\n",
      " -4.52390686e-02 -1.78862456e-02 -1.29444927e-01  1.78535562e-02\n",
      " -1.35572582e-01  7.55729899e-02 -1.03446893e-01 -2.47793589e-02\n",
      "  1.64004609e-01 -1.21331766e-01  6.76598847e-02 -2.01190710e-02\n",
      "  1.56776443e-01 -2.02018216e-01  1.02364056e-01 -2.39968732e-01\n",
      " -1.05118610e-01  2.62124628e-01  1.00840524e-01 -4.63756733e-02\n",
      " -9.92745087e-02 -9.49116871e-02  4.90577817e-02  5.04905358e-02\n",
      " -5.54871932e-02  2.47450605e-01 -2.12075561e-02 -9.24019068e-02\n",
      " -2.77189724e-02  6.13165013e-02  1.07418664e-01 -2.53350586e-01\n",
      "  1.06745504e-01 -1.23217389e-01  1.52054563e-01  1.14129083e-02\n",
      "  1.13400117e-01  6.62953928e-02  1.34087101e-01  1.99471503e-01\n",
      " -2.41768192e-02 -5.50174490e-02  1.09722771e-01 -2.23909363e-01\n",
      "  8.26420113e-02  2.99111065e-02  1.24859758e-01  2.37413142e-02\n",
      "  8.53732005e-02 -3.04297283e-02 -1.55957744e-01 -1.96234524e-01\n",
      " -7.55985752e-02  4.28691804e-02  3.87122072e-02  1.08364113e-01\n",
      "  3.96271721e-02 -9.84789245e-03  1.93227664e-01  5.57100996e-02\n",
      " -2.92386375e-02 -1.13342740e-01 -6.58899371e-04  4.72931564e-02\n",
      "  1.46984026e-01  1.35594249e-01 -1.47479638e-01  2.62801889e-02\n",
      "  7.18695447e-02  2.55204160e-02  1.15503836e-03 -1.82698384e-01\n",
      " -1.14952773e-01 -1.39721602e-01  2.17805535e-01 -2.26227403e-01\n",
      " -9.78788547e-03  2.04969943e-02 -2.32485816e-01 -6.57813475e-02\n",
      "  2.34956946e-02 -8.72668549e-02 -1.13214262e-01 -3.25393043e-02\n",
      " -3.48132737e-02  6.79193139e-02  1.44262239e-01 -6.92566633e-02\n",
      "  6.23751618e-02  1.05849355e-01  1.40487835e-01  3.00772442e-03\n",
      " -7.31809139e-02  1.42345086e-01 -6.21621050e-02  1.07868671e-01\n",
      "  4.80147116e-02 -4.40090075e-02 -1.03755295e-01 -1.03169180e-01\n",
      " -8.16521496e-02  3.99090312e-02 -1.77603159e-02 -2.77268458e-02\n",
      " -7.47294351e-02 -1.12457894e-01 -5.78650879e-03 -1.28012091e-01\n",
      "  1.75321266e-01  1.15290582e-01  4.95752431e-02 -1.92108542e-01\n",
      "  1.32259771e-01 -5.04196621e-02 -1.18748911e-01 -4.46391031e-02\n",
      " -1.22405596e-01  3.84257101e-02  2.73020603e-02  9.27861184e-02\n",
      " -1.39609933e-01 -7.95054659e-02  4.01469544e-02  1.42288171e-02\n",
      "  7.46442452e-02  1.27285644e-01 -1.78638205e-01 -2.25362834e-03\n",
      "  8.95239599e-03  4.20790277e-02 -1.00718670e-01  2.21897420e-02\n",
      "  2.65642554e-02  5.45990467e-02 -3.30613665e-02  1.77723035e-01\n",
      " -2.86637526e-02  8.82235914e-02 -9.52571109e-02  2.48353798e-02\n",
      "  1.36387184e-01 -8.94467346e-03 -1.71865493e-01  4.41714488e-02\n",
      "  4.73996587e-02 -4.39219587e-02  9.77496337e-03 -5.42025408e-03\n",
      "  6.91667721e-02  3.59897539e-02 -5.36174215e-02 -1.31496638e-01\n",
      "  1.99612975e-01 -8.82329494e-02 -6.87246248e-02 -1.01051778e-02\n",
      " -1.88372061e-01 -2.07616836e-02 -1.36182413e-01  8.35063979e-02\n",
      " -7.11752996e-02  1.47536546e-01  1.06374167e-01  1.17134318e-01\n",
      "  1.36325017e-01 -1.23134367e-01  1.65895410e-02 -4.42964360e-02\n",
      "  1.55587494e-02  3.23768407e-02  1.42182400e-02 -1.13556422e-01\n",
      "  6.10476285e-02  4.63670343e-02 -1.48584098e-01  1.01433240e-01\n",
      " -8.35030749e-02  2.18464479e-01 -3.24000455e-02 -2.09675416e-01\n",
      " -1.18011847e-01  7.47703807e-03 -6.45524859e-02  2.96471808e-02\n",
      "  1.53077975e-01  6.48091584e-02 -2.29209997e-02 -6.31712824e-02\n",
      "  1.75687432e-01 -2.61072274e-02 -1.20140538e-02  1.58894300e-01\n",
      "  1.74838863e-02 -6.31640255e-02  1.45852044e-01 -2.77172141e-02\n",
      "  5.90916956e-03 -3.72284055e-02 -8.48398283e-02  4.00620289e-02\n",
      "  9.44976788e-03 -2.28480790e-02 -3.73958722e-02 -3.68800573e-02\n",
      " -1.34502985e-02 -1.83448672e-01  5.94825298e-02 -3.55474114e-01\n",
      "  5.10534570e-02 -9.62503999e-02  1.25675499e-01  3.25810052e-02\n",
      "  3.58168371e-02  1.43691376e-01 -9.65508744e-02  1.24825664e-01\n",
      "  1.13360077e-01  4.05411497e-02 -2.00582340e-01  1.05493821e-01\n",
      " -4.77551892e-02  2.01842226e-02  1.55817373e-02 -1.51060997e-02\n",
      " -1.51770515e-02 -9.19877663e-02  1.91900179e-01  7.31442794e-02\n",
      " -1.57328043e-02 -1.23390667e-01 -2.64467865e-01  3.68382898e-03\n",
      " -9.53095481e-02  4.23561595e-02  9.54830796e-02 -7.32086375e-02\n",
      " -3.16508755e-04 -1.17530733e-01 -1.58784956e-01  1.09968610e-01\n",
      " -1.02465838e-01 -4.82560359e-02  4.84322198e-02 -6.24865331e-02\n",
      " -2.64380220e-02  1.34425178e-01  6.96453452e-03 -2.81366780e-02\n",
      " -9.96555761e-02 -1.68830216e-01  1.04691222e-01 -9.38411579e-02\n",
      "  9.31386724e-02 -2.29917884e-01  9.69728753e-02 -1.79637805e-01\n",
      "  2.40822807e-02 -1.98926806e-01 -7.53393099e-02  1.24233536e-01\n",
      " -6.25405014e-02  5.23645058e-02  2.11183336e-02 -6.82456419e-02\n",
      "  1.15945943e-01  9.53359082e-02  4.19052690e-02 -1.59507632e-01\n",
      "  7.20795095e-02 -2.90335596e-01  4.73032258e-02  1.44251764e-01\n",
      "  1.81037664e-01 -1.76705018e-01  4.97727394e-02 -3.03579792e-02\n",
      " -9.95159000e-02 -1.18401438e-01  8.75195581e-03  1.38810575e-01\n",
      "  1.31270438e-01 -1.59130797e-01 -3.12877968e-02 -1.98671401e-01\n",
      " -4.64865044e-02  1.70839936e-01 -6.43984005e-02 -1.85400188e-01\n",
      "  5.27466722e-02 -6.28370792e-02  8.80038440e-02 -4.85505648e-02\n",
      " -4.52055503e-03 -1.55364394e-01 -1.21277496e-01  9.41563100e-02\n",
      "  1.84189826e-02 -5.96052706e-02  6.07096255e-02  9.38019603e-02\n",
      "  6.98997751e-02  2.84302324e-01 -1.90200120e-01  3.95006798e-02\n",
      "  1.20244168e-01  5.94365001e-02 -3.58289331e-02  8.54341835e-02\n",
      " -7.49055967e-02 -4.70984690e-02 -1.07677467e-01  1.84469417e-01\n",
      "  8.32513645e-02 -1.65517423e-02 -1.16837053e-02 -2.20040344e-02\n",
      "  3.42992768e-02 -1.52433544e-01 -6.44963905e-02  6.89864233e-02\n",
      " -5.31540327e-02 -9.32473224e-03 -1.24259412e-01  7.46512264e-02\n",
      " -6.11004382e-02 -1.26144409e-01  1.15239844e-01 -5.64705692e-02\n",
      "  7.22486526e-02  1.57940790e-01  1.96265839e-02  6.48038983e-02\n",
      "  1.73934951e-01  1.93063557e-01  6.77423254e-02  6.56032860e-02\n",
      "  3.77760753e-02  4.58960868e-02 -5.74473888e-02  1.20214224e-01\n",
      "  1.24085359e-01 -5.30850329e-02 -7.33606368e-02  2.22614799e-02\n",
      " -9.29119810e-02  2.03116357e-01 -1.14664182e-01  9.24309045e-02\n",
      " -1.71670318e-01  5.80528900e-02 -5.93576804e-02  2.52290219e-02\n",
      " -3.07829261e-01 -2.27831118e-02 -1.05067641e-01  2.46909693e-01\n",
      "  4.51894850e-02  7.38373026e-02  3.38386893e-02  6.29883185e-02\n",
      "  3.31688230e-03  1.39870830e-02 -7.08048269e-02 -1.41085595e-01\n",
      " -1.18766479e-01 -8.97424519e-02 -6.65146112e-02  8.55316781e-03\n",
      " -1.08809575e-01 -1.08134374e-01 -4.42913808e-02 -2.32425751e-03\n",
      "  3.37513424e-02  8.68410803e-03 -2.05360383e-01 -2.73067176e-01]\n"
     ]
    }
   ],
   "source": [
    "word_vector = trained_model.wv['machine']\n",
    "print(f\"Vector for 'Object':\\n{word_vector}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('learning', 0.9674617052078247), ('ai', 0.9661654233932495), ('signal', 0.9635133147239685), ('facial', 0.9585362672805786), ('expressions', 0.956897497177124), ('responsible', 0.9527624845504761), ('face', 0.9480258822441101), ('analysis', 0.9473348259925842), ('maturing', 0.9462795257568359), ('automatic', 0.9373029470443726)]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    similar_words = load_model.wv.most_similar(positive=['machine'], topn=10)\n",
    "    print(similar_words)\n",
    "except KeyError:\n",
    "    print(\"'word' not found in vocabulary.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "    <style>\n",
       "        .bk-notebook-logo {\n",
       "            display: block;\n",
       "            width: 20px;\n",
       "            height: 20px;\n",
       "            background-image: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAABx0RVh0U29mdHdhcmUAQWRvYmUgRmlyZXdvcmtzIENTNui8sowAAAOkSURBVDiNjZRtaJVlGMd/1/08zzln5zjP1LWcU9N0NkN8m2CYjpgQYQXqSs0I84OLIC0hkEKoPtiH3gmKoiJDU7QpLgoLjLIQCpEsNJ1vqUOdO7ppbuec5+V+rj4ctwzd8IIbbi6u+8f1539dt3A78eXC7QizUF7gyV1fD1Yqg4JWz84yffhm0qkFqBogB9rM8tZdtwVsPUhWhGcFJngGeWrPzHm5oaMmkfEg1usvLFyc8jLRqDOMru7AyC8saQr7GG7f5fvDeH7Ej8CM66nIF+8yngt6HWaKh7k49Soy9nXurCi1o3qUbS3zWfrYeQDTB/Qj6kX6Ybhw4B+bOYoLKCC9H3Nu/leUTZ1JdRWkkn2ldcCamzrcf47KKXdAJllSlxAOkRgyHsGC/zRday5Qld9DyoM4/q/rUoy/CXh3jzOu3bHUVZeU+DEn8FInkPBFlu3+nW3Nw0mk6vCDiWg8CeJaxEwuHS3+z5RgY+YBR6V1Z1nxSOfoaPa4LASWxxdNp+VWTk7+4vzaou8v8PN+xo+KY2xsw6une2frhw05CTYOmQvsEhjhWjn0bmXPjpE1+kplmmkP3suftwTubK9Vq22qKmrBhpY4jvd5afdRA3wGjFAgcnTK2s4hY0/GPNIb0nErGMCRxWOOX64Z8RAC4oCXdklmEvcL8o0BfkNK4lUg9HTl+oPlQxdNo3Mg4Nv175e/1LDGzZen30MEjRUtmXSfiTVu1kK8W4txyV6BMKlbgk3lMwYCiusNy9fVfvvwMxv8Ynl6vxoByANLTWplvuj/nF9m2+PDtt1eiHPBr1oIfhCChQMBw6Aw0UulqTKZdfVvfG7VcfIqLG9bcldL/+pdWTLxLUy8Qq38heUIjh4XlzZxzQm19lLFlr8vdQ97rjZVOLf8nclzckbcD4wxXMidpX30sFd37Fv/GtwwhzhxGVAprjbg0gCAEeIgwCZyTV2Z1REEW8O4py0wsjeloKoMr6iCY6dP92H6Vw/oTyICIthibxjm/DfN9lVz8IqtqKYLUXfoKVMVQVVJOElGjrnnUt9T9wbgp8AyYKaGlqingHZU/uG2NTZSVqwHQTWkx9hxjkpWDaCg6Ckj5qebgBVbT3V3NNXMSiWSDdGV3hrtzla7J+duwPOToIg42ChPQOQjspnSlp1V+Gjdged7+8UN5CRAV7a5EdFNwCjEaBR27b3W890TE7g24NAP/mMDXRWrGoFPQI9ls/MWO2dWFAar/xcOIImbbpA3zgAAAABJRU5ErkJggg==);\n",
       "        }\n",
       "    </style>\n",
       "    <div>\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-notebook-logo\"></a>\n",
       "        <span id=\"f0aaa7e8-be04-47c8-9671-ff6d958fa957\">Loading BokehJS ...</span>\n",
       "    </div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "'use strict';\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  const force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\nconst JS_MIME_TYPE = 'application/javascript';\n  const HTML_MIME_TYPE = 'text/html';\n  const EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n  const CLASS_NAME = 'output_bokeh rendered_html';\n\n  /**\n   * Render data to the DOM node\n   */\n  function render(props, node) {\n    const script = document.createElement(\"script\");\n    node.appendChild(script);\n  }\n\n  /**\n   * Handle when an output is cleared or removed\n   */\n  function handleClearOutput(event, handle) {\n    function drop(id) {\n      const view = Bokeh.index.get_by_id(id)\n      if (view != null) {\n        view.model.document.clear()\n        Bokeh.index.delete(view)\n      }\n    }\n\n    const cell = handle.cell;\n\n    const id = cell.output_area._bokeh_element_id;\n    const server_id = cell.output_area._bokeh_server_id;\n\n    // Clean up Bokeh references\n    if (id != null) {\n      drop(id)\n    }\n\n    if (server_id !== undefined) {\n      // Clean up Bokeh references\n      const cmd_clean = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n      cell.notebook.kernel.execute(cmd_clean, {\n        iopub: {\n          output: function(msg) {\n            const id = msg.content.text.trim()\n            drop(id)\n          }\n        }\n      });\n      // Destroy server and session\n      const cmd_destroy = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n      cell.notebook.kernel.execute(cmd_destroy);\n    }\n  }\n\n  /**\n   * Handle when a new output is added\n   */\n  function handleAddOutput(event, handle) {\n    const output_area = handle.output_area;\n    const output = handle.output;\n\n    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n    if ((output.output_type != \"display_data\") || (!Object.prototype.hasOwnProperty.call(output.data, EXEC_MIME_TYPE))) {\n      return\n    }\n\n    const toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n\n    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n      // store reference to embed id on output_area\n      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n    }\n    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n      const bk_div = document.createElement(\"div\");\n      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n      const script_attrs = bk_div.children[0].attributes;\n      for (let i = 0; i < script_attrs.length; i++) {\n        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n      }\n      // store reference to server id on output_area\n      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n    }\n  }\n\n  function register_renderer(events, OutputArea) {\n\n    function append_mime(data, metadata, element) {\n      // create a DOM node to render to\n      const toinsert = this.create_output_subarea(\n        metadata,\n        CLASS_NAME,\n        EXEC_MIME_TYPE\n      );\n      this.keyboard_manager.register_events(toinsert);\n      // Render to node\n      const props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n      render(props, toinsert[toinsert.length - 1]);\n      element.append(toinsert);\n      return toinsert\n    }\n\n    /* Handle when an output is cleared or removed */\n    events.on('clear_output.CodeCell', handleClearOutput);\n    events.on('delete.Cell', handleClearOutput);\n\n    /* Handle when a new output is added */\n    events.on('output_added.OutputArea', handleAddOutput);\n\n    /**\n     * Register the mime type and append_mime function with output_area\n     */\n    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n      /* Is output safe? */\n      safe: true,\n      /* Index of renderer in `output_area.display_order` */\n      index: 0\n    });\n  }\n\n  // register the mime type if in Jupyter Notebook environment and previously unregistered\n  if (root.Jupyter !== undefined) {\n    const events = require('base/js/events');\n    const OutputArea = require('notebook/js/outputarea').OutputArea;\n\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  }\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  const NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded(error = null) {\n    const el = document.getElementById(\"f0aaa7e8-be04-47c8-9671-ff6d958fa957\");\n    if (el != null) {\n      const html = (() => {\n        if (typeof root.Bokeh === \"undefined\") {\n          if (error == null) {\n            return \"BokehJS is loading ...\";\n          } else {\n            return \"BokehJS failed to load.\";\n          }\n        } else {\n          const prefix = `BokehJS ${root.Bokeh.version}`;\n          if (error == null) {\n            return `${prefix} successfully loaded.`;\n          } else {\n            return `${prefix} <b>encountered errors</b> while loading and may not function as expected.`;\n          }\n        }\n      })();\n      el.innerHTML = html;\n\n      if (error != null) {\n        const wrapper = document.createElement(\"div\");\n        wrapper.style.overflow = \"auto\";\n        wrapper.style.height = \"5em\";\n        wrapper.style.resize = \"vertical\";\n        const content = document.createElement(\"div\");\n        content.style.fontFamily = \"monospace\";\n        content.style.whiteSpace = \"pre-wrap\";\n        content.style.backgroundColor = \"rgb(255, 221, 221)\";\n        content.textContent = error.stack ?? error.toString();\n        wrapper.append(content);\n        el.append(wrapper);\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(() => display_loaded(error), 100);\n    }\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error(url) {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.4.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.4.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.4.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.4.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-3.4.0.min.js\"];\n  const css_urls = [];\n\n  const inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {\n    }\n  ];\n\n  function run_inline_js() {\n    if (root.Bokeh !== undefined || force === true) {\n      try {\n            for (let i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n\n      } catch (error) {display_loaded(error);throw error;\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      const cell = $(document.getElementById(\"f0aaa7e8-be04-47c8-9671-ff6d958fa957\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));",
      "application/vnd.bokehjs_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"f75ffb9a-90e6-4361-9b15-3bf49d586711\" data-root-id=\"p1075\" style=\"display: contents;\"></div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "(function(root) {\n  function embed_document(root) {\n  const docs_json = {\"1d6506e6-5a50-41ec-982f-d1974d3f1072\":{\"version\":\"3.4.0\",\"title\":\"Bokeh Application\",\"roots\":[{\"type\":\"object\",\"name\":\"Figure\",\"id\":\"p1075\",\"attributes\":{\"x_range\":{\"type\":\"object\",\"name\":\"Range1d\",\"id\":\"p1085\",\"attributes\":{\"start\":-1.5,\"end\":1.5}},\"y_range\":{\"type\":\"object\",\"name\":\"Range1d\",\"id\":\"p1086\",\"attributes\":{\"start\":-1.5,\"end\":1.5}},\"x_scale\":{\"type\":\"object\",\"name\":\"LinearScale\",\"id\":\"p1087\"},\"y_scale\":{\"type\":\"object\",\"name\":\"LinearScale\",\"id\":\"p1088\"},\"title\":{\"type\":\"object\",\"name\":\"Title\",\"id\":\"p1078\",\"attributes\":{\"text\":\"Thesis Knowledge Graph\"}},\"renderers\":[{\"type\":\"object\",\"name\":\"GraphRenderer\",\"id\":\"p1111\",\"attributes\":{\"layout_provider\":{\"type\":\"object\",\"name\":\"StaticLayoutProvider\",\"id\":\"p1128\",\"attributes\":{\"graph_layout\":{\"type\":\"map\",\"entries\":[[0,{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"D3a1Sa7P2b+WfuI2+fHIvw==\"},\"shape\":[2],\"dtype\":\"float64\",\"order\":\"little\"}],[1,{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"UmZKFYAstb87EW8O1su2vw==\"},\"shape\":[2],\"dtype\":\"float64\",\"order\":\"little\"}],[2,{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"3SaC5oStmb8YbKqolACgPw==\"},\"shape\":[2],\"dtype\":\"float64\",\"order\":\"little\"}],[3,{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"mumtM0166j/Ljc0PvgTOvw==\"},\"shape\":[2],\"dtype\":\"float64\",\"order\":\"little\"}],[4,{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"tsncz+7B6j80MO3lYnHYvw==\"},\"shape\":[2],\"dtype\":\"float64\",\"order\":\"little\"}],[5,{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"EKolEtvZ3z8FxjzivwvGvw==\"},\"shape\":[2],\"dtype\":\"float64\",\"order\":\"little\"}],[6,{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"vcNukhRL5T85c/+/MaHdvw==\"},\"shape\":[2],\"dtype\":\"float64\",\"order\":\"little\"}],[7,{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"nNcEfpEZ2j/TWhFfa6vSvw==\"},\"shape\":[2],\"dtype\":\"float64\",\"order\":\"little\"}],[8,{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"8WiUXl4t5D8Q2aVAPIfCvw==\"},\"shape\":[2],\"dtype\":\"float64\",\"order\":\"little\"}],[9,{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"ih+/6t/X7z9k2Mv52SPCvw==\"},\"shape\":[2],\"dtype\":\"float64\",\"order\":\"little\"}],[10,{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"Y6vB0wJl4L9dr3a49hfXvw==\"},\"shape\":[2],\"dtype\":\"float64\",\"order\":\"little\"}],[11,{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"mcS2CXKH6L+dT2y+J1Phvw==\"},\"shape\":[2],\"dtype\":\"float64\",\"order\":\"little\"}],[12,{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"AAAAAAAA8L+nkTf9TxTjvw==\"},\"shape\":[2],\"dtype\":\"float64\",\"order\":\"little\"}],[13,{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"1AeyJ5GL0r8nB4kU+vbvPw==\"},\"shape\":[2],\"dtype\":\"float64\",\"order\":\"little\"}],[14,{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"G8TE+UyC179twfnOhrvlPw==\"},\"shape\":[2],\"dtype\":\"float64\",\"order\":\"little\"}],[15,{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"era7Dnnnw79K92m7N2vfPw==\"},\"shape\":[2],\"dtype\":\"float64\",\"order\":\"little\"}],[16,{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"IGxC18dy1b/kFttmSwLNPw==\"},\"shape\":[2],\"dtype\":\"float64\",\"order\":\"little\"}],[17,{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"30BnOJ6N078D/bA7pe/dPw==\"},\"shape\":[2],\"dtype\":\"float64\",\"order\":\"little\"}],[18,{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"ZssxGTKQxr8b4zpjYVvUPw==\"},\"shape\":[2],\"dtype\":\"float64\",\"order\":\"little\"}],[19,{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"3TRMMeAz3L/mNCkAKpLZPw==\"},\"shape\":[2],\"dtype\":\"float64\",\"order\":\"little\"}]]}}},\"node_renderer\":{\"type\":\"object\",\"name\":\"GlyphRenderer\",\"id\":\"p1116\",\"attributes\":{\"data_source\":{\"type\":\"object\",\"name\":\"ColumnDataSource\",\"id\":\"p1113\",\"attributes\":{\"selected\":{\"type\":\"object\",\"name\":\"Selection\",\"id\":\"p1114\",\"attributes\":{\"indices\":[],\"line_indices\":[]}},\"selection_policy\":{\"type\":\"object\",\"name\":\"UnionRenderers\",\"id\":\"p1115\"},\"data\":{\"type\":\"map\",\"entries\":[[\"title\",[\"Communicating Ethics across the AI Ecosystem\",\"Automatic Facial Expression Analysis\",\"Machine Learning: A maturing field\",\"Edge Assisted Real-Time Object Detection for Mobile and augmented Reality\",\"You Only Look Once: Unified, Real-Time Object Detection\",\"Deep learning\",\"A survey on Image Data Augmentation for Deep Learning\",\"Towards Deep Learning Models Resistant to Adversarial Attacks\",\"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\",\"GLIMPSE: Continuous, Real-Time Object Recognition on Mobile Devices\",\"Cost-Effectiveness in Health and Medicine (2nd edn)\",\"Beyond the bedside: Clinicians as guardians of public health, medicine and science\",\"Free Radicals in Biology and Medicine (5th edn)\",\"Global food demand and the sustainable intensification of agriculture\",\"The Food and Agriculture Organization of the United Nations\",\"Style opportunity commercial question organization population feel.\",\"Claim laugh establish have year design Republican.\",\"Main left see two thus hand indicate.\",\"Imagine all usually strong.\",\"Maybe from indicate music.\"]],[\"index\",[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19]]]}}},\"view\":{\"type\":\"object\",\"name\":\"CDSView\",\"id\":\"p1117\",\"attributes\":{\"filter\":{\"type\":\"object\",\"name\":\"AllIndices\",\"id\":\"p1118\"}}},\"glyph\":{\"type\":\"object\",\"name\":\"Scatter\",\"id\":\"p1112\"}}},\"edge_renderer\":{\"type\":\"object\",\"name\":\"GlyphRenderer\",\"id\":\"p1123\",\"attributes\":{\"data_source\":{\"type\":\"object\",\"name\":\"ColumnDataSource\",\"id\":\"p1120\",\"attributes\":{\"selected\":{\"type\":\"object\",\"name\":\"Selection\",\"id\":\"p1121\",\"attributes\":{\"indices\":[],\"line_indices\":[]}},\"selection_policy\":{\"type\":\"object\",\"name\":\"UnionRenderers\",\"id\":\"p1122\"},\"data\":{\"type\":\"map\",\"entries\":[[\"weight\",[0.9844935056014805,0.943170535017979,0.8608175522448814,0.7201791591462683,0.7050466981912877,0.9617303996514937,0.7013046269904222,0.7055678947739807,0.8426433848086639,0.7172481133262656,0.7237178055047887,0.705638979136951,0.7185917883067152,0.7683205079123799,0.7092386303447289,0.8500722738520441,0.729364423113483,0.7598923331100809,0.7473519498804491,0.7586957555141158,0.7063904038715196,0.9678851702554354,0.841569836392484,0.7623348880119059,0.7418195511831929,0.8423720086340123,0.9457751098414063,0.8997787350221177,0.8042299482594604,0.7564438032240896,0.8794000230733245,0.9518432481167319,0.9582074754989676,0.9101589522441031,0.983251894808783,0.8015921039424729,0.9616364657916474,0.9511469757217013,0.9287735646628708,0.7905123238170959,0.9181864173982338,0.8352958106791897,0.9088766183719282,0.9585285823053903,0.7733573360279339,0.902766764837073,0.7920580246112006,0.7197389668431239,0.7360390604532996,0.7670642149313311,0.9507784568422193,0.9238806381009803,0.9259161083630363,0.9212618003138946,0.9822493501060617,0.9940687757927666,0.9797719675004314,0.9898289226819048,0.9804574308932732,0.9817179788804974]],[\"start\",[0,0,0,0,0,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,3,3,3,3,3,3,4,4,4,4,4,5,5,5,5,6,6,7,8,10,10,11,13,13,14,14,14,14,14,15,15,15,15,16,16,16,17,17,18]],[\"end\",[1,2,10,11,16,2,5,7,10,15,16,18,5,7,8,10,15,16,17,18,19,4,5,6,7,8,9,5,6,7,8,9,6,7,8,9,7,8,8,9,11,12,12,14,15,15,16,17,18,19,16,17,18,19,17,18,19,18,19,19]]]}}},\"view\":{\"type\":\"object\",\"name\":\"CDSView\",\"id\":\"p1124\",\"attributes\":{\"filter\":{\"type\":\"object\",\"name\":\"AllIndices\",\"id\":\"p1125\"}}},\"glyph\":{\"type\":\"object\",\"name\":\"MultiLine\",\"id\":\"p1119\"}}},\"selection_policy\":{\"type\":\"object\",\"name\":\"NodesOnly\",\"id\":\"p1126\"},\"inspection_policy\":{\"type\":\"object\",\"name\":\"NodesOnly\",\"id\":\"p1127\"}}}],\"toolbar\":{\"type\":\"object\",\"name\":\"Toolbar\",\"id\":\"p1084\",\"attributes\":{\"tools\":[{\"type\":\"object\",\"name\":\"PanTool\",\"id\":\"p1099\"},{\"type\":\"object\",\"name\":\"WheelZoomTool\",\"id\":\"p1100\",\"attributes\":{\"renderers\":\"auto\"}},{\"type\":\"object\",\"name\":\"BoxZoomTool\",\"id\":\"p1101\",\"attributes\":{\"overlay\":{\"type\":\"object\",\"name\":\"BoxAnnotation\",\"id\":\"p1102\",\"attributes\":{\"syncable\":false,\"level\":\"overlay\",\"visible\":false,\"left\":{\"type\":\"number\",\"value\":\"nan\"},\"right\":{\"type\":\"number\",\"value\":\"nan\"},\"top\":{\"type\":\"number\",\"value\":\"nan\"},\"bottom\":{\"type\":\"number\",\"value\":\"nan\"},\"left_units\":\"canvas\",\"right_units\":\"canvas\",\"top_units\":\"canvas\",\"bottom_units\":\"canvas\",\"line_color\":\"black\",\"line_alpha\":1.0,\"line_width\":2,\"line_dash\":[4,4],\"fill_color\":\"lightgrey\",\"fill_alpha\":0.5}}}},{\"type\":\"object\",\"name\":\"ResetTool\",\"id\":\"p1107\"},{\"type\":\"object\",\"name\":\"HoverTool\",\"id\":\"p1108\",\"attributes\":{\"renderers\":\"auto\"}},{\"type\":\"object\",\"name\":\"SaveTool\",\"id\":\"p1109\"},{\"type\":\"object\",\"name\":\"HoverTool\",\"id\":\"p1110\",\"attributes\":{\"renderers\":\"auto\",\"tooltips\":[[\"Title\",\"@title\"]]}}]}},\"left\":[{\"type\":\"object\",\"name\":\"LinearAxis\",\"id\":\"p1094\",\"attributes\":{\"ticker\":{\"type\":\"object\",\"name\":\"BasicTicker\",\"id\":\"p1095\",\"attributes\":{\"mantissas\":[1,2,5]}},\"formatter\":{\"type\":\"object\",\"name\":\"BasicTickFormatter\",\"id\":\"p1096\"},\"major_label_policy\":{\"type\":\"object\",\"name\":\"AllLabels\",\"id\":\"p1097\"}}}],\"below\":[{\"type\":\"object\",\"name\":\"LinearAxis\",\"id\":\"p1089\",\"attributes\":{\"ticker\":{\"type\":\"object\",\"name\":\"BasicTicker\",\"id\":\"p1090\",\"attributes\":{\"mantissas\":[1,2,5]}},\"formatter\":{\"type\":\"object\",\"name\":\"BasicTickFormatter\",\"id\":\"p1091\"},\"major_label_policy\":{\"type\":\"object\",\"name\":\"AllLabels\",\"id\":\"p1092\"}}}],\"center\":[{\"type\":\"object\",\"name\":\"Grid\",\"id\":\"p1093\",\"attributes\":{\"axis\":{\"id\":\"p1089\"}}},{\"type\":\"object\",\"name\":\"Grid\",\"id\":\"p1098\",\"attributes\":{\"dimension\":1,\"axis\":{\"id\":\"p1094\"}}}]}}]}};\n  const render_items = [{\"docid\":\"1d6506e6-5a50-41ec-982f-d1974d3f1072\",\"roots\":{\"p1075\":\"f75ffb9a-90e6-4361-9b15-3bf49d586711\"},\"root_ids\":[\"p1075\"]}];\n  void root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\n  }\n  if (root.Bokeh !== undefined) {\n    embed_document(root);\n  } else {\n    let attempts = 0;\n    const timer = setInterval(function(root) {\n      if (root.Bokeh !== undefined) {\n        clearInterval(timer);\n        embed_document(root);\n      } else {\n        attempts++;\n        if (attempts > 100) {\n          clearInterval(timer);\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\n        }\n      }\n    }, 10, root)\n  }\n})(window);",
      "application/vnd.bokehjs_exec.v0+json": ""
     },
     "metadata": {
      "application/vnd.bokehjs_exec.v0+json": {
       "id": "p1075"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import networkx as nx\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from bokeh.io import show, output_notebook\n",
    "from bokeh.plotting import figure, from_networkx\n",
    "from bokeh.models import HoverTool\n",
    "\n",
    "titles = []\n",
    "embeddings = []\n",
    "\n",
    "# Collect titles and embeddings\n",
    "for doc in collection.find():\n",
    "    title = doc.get('title')\n",
    "    embedding = doc.get('abstract_embedding')\n",
    "    if title is not None and embedding is not None:\n",
    "        titles.append(title)\n",
    "        embeddings.append(embedding)\n",
    "\n",
    "threshold = 0.7\n",
    "\n",
    "# Build graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add nodes (titles)\n",
    "for i, title in enumerate(titles):\n",
    "    G.add_node(i, title=title)\n",
    "\n",
    "# Add edges based on similarity\n",
    "num_titles = len(titles)\n",
    "for i in range(num_titles):\n",
    "    for j in range(i + 1, num_titles):\n",
    "        similarity = cosine_similarity(np.array(embeddings[i]).reshape(1, -1), np.array(embeddings[j]).reshape(1, -1))[0][0]\n",
    "        if similarity > threshold:\n",
    "            G.add_edge(i, j, weight=similarity)\n",
    "\n",
    "# Create Bokeh plot\n",
    "plot = figure(title=\"Thesis Knowledge Graph\", x_range=(-1.5, 1.5), y_range=(-1.5, 1.5),\n",
    "              tools=\"pan,wheel_zoom,box_zoom,reset,hover,save\")\n",
    "\n",
    "# Add hover tool\n",
    "hover = HoverTool()\n",
    "hover.tooltips = [(\"Title\", \"@title\")]\n",
    "plot.add_tools(hover)\n",
    "\n",
    "# Convert NetworkX graph to Bokeh graph\n",
    "graph_renderer = from_networkx(G, nx.spring_layout, scale=1, center=(0, 0))\n",
    "plot.renderers.append(graph_renderer)\n",
    "\n",
    "# Show plot\n",
    "output_notebook()\n",
    "show(plot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embedding(text: str, model) -> List[float]:\n",
    "    words = text.split()\n",
    "    embedding_dim = model.vector_size\n",
    "    embeddings = np.zeros((len(words), embedding_dim))\n",
    "    for i, word in enumerate(words):\n",
    "        if word in model.wv:\n",
    "            embeddings[i] = model.wv[word]\n",
    "    if embeddings.shape[0] > 0:\n",
    "        embedding = np.mean(embeddings, axis=0)\n",
    "    else:\n",
    "        embedding = np.zeros(embedding_dim)\n",
    "    return embedding.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Machine learning above 2019\"\n",
    "vector_query = "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
