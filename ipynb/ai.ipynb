{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import nltk\n",
    "import pymongo\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from typing import List\n",
    "import numpy as np\n",
    "import gensim.models as gm\n",
    "import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to MongoDB\n",
    "client = pymongo.MongoClient(\"mongodb+srv://cedrickchu123:lzuaguRde81CZVuD@cluster0.75dzsfe.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0\")\n",
    "db = client.ThesisProject\n",
    "collection = db.Thesis_Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chat with an intelligent assistant in your terminal\n",
    "from openai import OpenAI\n",
    "import json\n",
    "client = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"not-needed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embedding(text: str, model) -> List[float]:\n",
    "    words = text.split()\n",
    "    embedding_dim = model.vector_size\n",
    "    embeddings = np.zeros((len(words), embedding_dim))\n",
    "    for i, word in enumerate(words):\n",
    "        if word in model.wv:\n",
    "            embeddings[i] = model.wv[word]\n",
    "    if embeddings.shape[0] > 0:\n",
    "        embedding = np.mean(embeddings, axis=0)\n",
    "    else:\n",
    "        embedding = np.zeros(embedding_dim)\n",
    "    return embedding.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model = gm.Word2Vec.load('../models/word2vec_model.gensim')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_keyword = \"Researches from 2020 to 2024\"\n",
    "vector_of_input_keyword = generate_embedding(input_keyword, load_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_date = [doc['date_published'] for doc in collection.find()]\n",
    "all_author = [doc['author'] for doc in collection.find()]\n",
    "all_title = [doc['title'] for doc in collection.find()]\n",
    "\n",
    "\n",
    "\n",
    "published_date = []\n",
    "for date in all_date:\n",
    "    formatted_date = date.strftime(\"%Y-%m-%d\")  \n",
    "    published_date.append(formatted_date)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I have data in MongoDB of all Thesis papers with their title, author, published_date,keywords and abstract .\\npublished_date can be anything from 2009-05-21\\nto 2024-12-16\\nauthors are [\\'Josephine Seah\\', \\'Michel Valstar\\', \\'Jaime Carbonell\\', \\'\"Marco Gruteser\\', \\'Joseph Redmon\\', \\'Yann LeCun , Yoshua Bengio , Geoffrey Hinton \\', \\' Taghi M. Khoshgoftaar \\', \\'Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, Adrian Vladu\\', \\'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby\\', \\'Tiffany Yu-Han Chen, Hari Balakrishnan, Lenin Ravindranath, Paramvir Bahl\\', \\'Peter J. Neumann (ed.), Theodore G. Ganiats (ed.), Louise B. Russell (ed.), Gillian D. Sanders (ed.), Joanna E. Siegel (ed.)\\', \\'Anish K Agarwal\\', \\'Barry Halliwell\\', \\'David Tilman\\', \\'H. Belshaw\\', \\'Amanda Medina\\', \\'Joseph Best\\', \\'Carl Robinson\\', \\'Erin Wong\\', \\'Antonio Nelson\\', \\'Rita Bailey\\', \\'Natasha Cobb\\', \\'Mr. Jason Anderson\\', \\'Denise White\\', \\'Donna Brown\\', \\'Tammy Mcbride\\', \\'Michael Gardner\\', \\'Melissa Nguyen\\', \\'Brad Adams\\', \\'Terry Hall\\', \\'Brian Roberts\\', \\'Clinton Jordan\\', \\'Douglas Allen\\', \\'Mary Ward\\', \\'Kara Kramer\\', \\'Isabella Hardin\\', \\'Matthew Williams\\']\\nbased on user\\'s search query. give me json output as follows\\n{\\n\"author\": \"it should be what users want.  give \\'Not-Mentioned\\' if user did not explicitly mentioned an author, if an author is mentioned by user is not in the the author list, give Not-Found\",\\n\"min_date\":\\n\"max_date\":\\n\\n}\\nusers query : Researches from 2020 to 2024\\n'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_date = min(published_date)\n",
    "max_date = max(published_date)\n",
    "\n",
    "my_prompt = f\"\"\"I have data in MongoDB of all Thesis papers with their title, author, published_date,keywords and abstract .\n",
    "published_date can be anything from 2009-05-21\n",
    "to 2024-12-16\n",
    "authors are {all_author}\n",
    "based on user's search query. give me json output as follows\n",
    "{{\n",
    "\"author\": \"it should be what users want.  give 'Not-Mentioned' if user did not explicitly mentioned an author, if an author is mentioned by user is not in the the author list, give Not-Found\",\n",
    "\"min_date\":\n",
    "\"max_date\":\n",
    "\n",
    "}}\n",
    "users query : {input_keyword}\n",
    "\"\"\"\n",
    "my_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-csiob4foq1e4rrur0l6ka7', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=' {\\n\"author\": \"Not Found\",\\n\"min\\\\_date\": \"2020-01-01T00:00:00Z\",\\n\"max\\\\_date\": \"2024-12-31T23:59:59Z\"\\n}', role='assistant', function_call=None, tool_calls=None))], created=1712051927, model='C:\\\\Users\\\\Ryzen\\\\.cache\\\\lm-studio\\\\models\\\\TheBloke\\\\Mistral-7B-Instruct-v0.1-GGUF\\\\mistral-7b-instruct-v0.1.Q3_K_M.gguf', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=67, prompt_tokens=2681, total_tokens=2748))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "  model=\"local-model\",\n",
    "  response_format={ \"type\": \"json_object\" },\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant designed to output only in JSON format. No other text or explaination.\"},\n",
    "    {\"role\": \"user\", \"content\": my_prompt}\n",
    "  ]\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_string = response.choices[0].message.content\n",
    "fixed_json_string = json_string.replace(\"\\\\\", \"\\\\\\\\\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31T23:59:59Z\n",
      "2020-01-01T00:00:00Z\n",
      "<class 'str'>\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "filter_map = json.loads(fixed_json_string)\n",
    "lt = filter_map['max\\\\_date']\n",
    "gt = filter_map['min\\\\_date']\n",
    "print(lt)\n",
    "print(gt)\n",
    "print(type(lt))\n",
    "print(type(gt))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'datetime' has no attribute 'fromisoformat'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[93], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpairwise\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cosine_similarity\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdatetime\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m lt \u001b[38;5;241m=\u001b[39m \u001b[43mdatetime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfromisoformat\u001b[49m(filter_map[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m+date\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      5\u001b[0m gt \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mfromisoformat(filter_map[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m_date\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      9\u001b[0m query_embedding \u001b[38;5;241m=\u001b[39m generate_embedding(input_keyword, load_model)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'datetime' has no attribute 'fromisoformat'"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import datetime\n",
    "\n",
    "lt = datetime.fromisoformat(filter_map['max\\\\+date'])\n",
    "gt = datetime.fromisoformat(filter_map['min\\\\_date'])\n",
    "\n",
    "\n",
    "\n",
    "query_embedding = generate_embedding(input_keyword, load_model)\n",
    "\n",
    "pipeline = [\n",
    "    {\"$search\": {\n",
    "        \"index\": \"index_mapping\",\n",
    "        \"compound\": {\n",
    "            \"must\": [{\"range\": {\"path\": \"date_published\", \"gt\": gt, \"lt\": lt}}],\n",
    "            \"should\": [{\"near\": {\"path\": \"date_published\", \"origin\": datetime.datetime(2015, 7, 1, 0, 0, 0, 0), \"pivot\": 2629800000}}],\n",
    "            }}},\n",
    "    {\"$limit\": 6},\n",
    "    {\"$project\": {\"_id\": 0, \"title\": 1, \"author\": 1, \"date_published\": 1, \"keywords\": 1, \"abstract\": 1, \"score\": {\"$meta\": \"searchScore\"}}}\n",
    "]\n",
    "\n",
    "search_results = list(collection.aggregate(pipeline))\n",
    "print(\"Search Results:\", search_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'author': 'Not Found', 'min\\\\_date': '2020-01-01T00:00:00Z', 'max\\\\_date': '2024-12-31T23:59:59Z'}\n"
     ]
    }
   ],
   "source": [
    "print(filter_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search Results: []\n"
     ]
    }
   ],
   "source": [
    "\n",
    "search_results = list(collection.aggregate(pipeline))\n",
    "print(\"Search Results:\", search_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Results: []\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
